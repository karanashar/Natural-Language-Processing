{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentence_polarity\n",
    "import random\n",
    "sentences = sentence_polarity.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['simplistic', ',', 'silly', 'and', 'tedious', '.'], 'neg')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [(sent, cat) for cat in sentence_polarity.categories() \n",
    "\tfor sent in sentence_polarity.sents(categories=cat)]\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ultimately'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words_list = [word for (sent,cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "word_items = all_words.most_common(2000)\n",
    "word_features = [word for (word,count) in word_items]\n",
    "word_features[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10662"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.723\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print (nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import os\n",
    "mycorpus = PlaintextCorpusReader('.', '.*\\.txt')\n",
    "mycorpus = mycorpus.raw('filename1.txt')\n",
    "token=nltk.sent_tokenize(mycorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=token[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[]\n",
    "n=[]\n",
    "for i in t:\n",
    "    temp=nltk.word_tokenize(i)\n",
    "    a=classifier.classify(document_features(temp,word_features))\n",
    "    if(a=='pos'):\n",
    "        p.append(i)\n",
    "    else:n.append(i)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418341\n",
      "722301\n"
     ]
    }
   ],
   "source": [
    "print(len(p))\n",
    "print(len(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'noone', 'rather', 'hardly', 'scarcely', 'rarely', 'seldom', 'neither', 'nor']\n",
    "\n",
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = False\n",
    "        features['contains(NOT{})'.format(word)] = False\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['contains({})'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_featuresets = [(NOT_features(d, word_features, negationwords), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.585\n"
     ]
    }
   ],
   "source": [
    "train_set1, test_set1 = NOT_featuresets[1000:], NOT_featuresets[:1000]\n",
    "classifier1 = nltk.NaiveBayesClassifier.train(train_set1)\n",
    "print (nltk.classify.accuracy(classifier1, test_set1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261.9031235694885\n"
     ]
    }
   ],
   "source": [
    "t1=time.time()\n",
    "p1=[]\n",
    "n1=[]\n",
    "for i in token:\n",
    "    temp=nltk.word_tokenize(i)\n",
    "    for j in temp:\n",
    "        if j.isalpha() and j not in stopwords:\n",
    "            continue\n",
    "        else: temp.remove(j)\n",
    "    a=classifier1.classify(NOT_features(temp,word_features,negationwords))\n",
    "    if(a=='pos'):\n",
    "        p1.append(i)\n",
    "    else:n1.append(i)\n",
    "t2=time.time()\n",
    "print((t2-t1)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1106600\n"
     ]
    }
   ],
   "source": [
    "print(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34042"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(n1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running Classifier on first featureset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierBonus=SklearnClassifier(BernoulliNB()).train(train_set)\n",
    "nltk.classify.accuracy(classifierBonus,test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running Classifier on second feature set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.585"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifierBonus=SklearnClassifier(BernoulliNB()).train(train_set1)\n",
    "nltk.classify.accuracy(classifierBonus,test_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
